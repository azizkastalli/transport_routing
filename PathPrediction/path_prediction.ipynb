{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aggressive-belly",
   "metadata": {},
   "source": [
    "## Correct path prediction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "senior-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "analyzed-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow .data import Dataset\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, RNN, GRU, LSTM, Dropout, BatchNormalization\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powered-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bored-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1950296 entries, 0 to 1950295\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   vehicle_id     int64  \n",
      " 1   line_id        int64  \n",
      " 2   latitude       float64\n",
      " 3   longitude      float64\n",
      " 4   datetime       object \n",
      " 5   station_id     int64  \n",
      " 6   vehicle_type   float64\n",
      " 7   sequence_id    object \n",
      " 8   order          int64  \n",
      " 9   line_label     object \n",
      " 10  datetime_diff  float64\n",
      " 11  outlier        int64  \n",
      "dtypes: float64(4), int64(5), object(3)\n",
      "memory usage: 178.6+ MB\n"
     ]
    }
   ],
   "source": [
    "picktime = pd.read_csv('../dataPreprocessing/data/gps_clean.csv', low_memory=False)\n",
    "picktime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blocked-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = dict( (target,code) for code, target in enumerate(picktime.line_id.unique()) )\n",
    "target_decoder = dict( (code, target) for target, code in target_encoder.items() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "british-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_targets = picktime.line_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "automotive-edward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "severe-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_lineid = picktime[['sequence_id','line_id']].groupby('sequence_id').first().line_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-visiting",
   "metadata": {},
   "source": [
    "### DATA ETL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accomplished-accommodation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------ETL------------------------------\n",
      "\n",
      "\n",
      "loading chunk  fullpick_chunk1 ... done in 6.37 second(s)\n",
      "chunk preprocessing ... done in 114.75 second(s)\n",
      "generating stop steps sequences ... done in 14.07 second(s)\n",
      "extracting preprocessed numpy array data ... done in 69.77 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9475562\n",
      "                          * data length       =  70785\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk10 ... done in 10.72 second(s)\n",
      "chunk preprocessing ... done in 78.22 second(s)\n",
      "generating stop steps sequences ... done in 9.81 second(s)\n",
      "extracting preprocessed numpy array data ... done in 12.18 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  6076372\n",
      "                          * data length       =  49717\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk11 ... done in 9.46 second(s)\n",
      "chunk preprocessing ... done in 115.76 second(s)\n",
      "generating stop steps sequences ... done in 14.98 second(s)\n",
      "extracting preprocessed numpy array data ... done in 52.71 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  10302028\n",
      "                          * data length       =  76259\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk12 ... done in 13.71 second(s)\n",
      "chunk preprocessing ... done in 118.27 second(s)\n",
      "generating stop steps sequences ... done in 15.20 second(s)\n",
      "extracting preprocessed numpy array data ... done in 40.38 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9959062\n",
      "                          * data length       =  74549\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk13 ... done in 12.90 second(s)\n",
      "chunk preprocessing ... done in 119.58 second(s)\n",
      "generating stop steps sequences ... done in 15.11 second(s)\n",
      "extracting preprocessed numpy array data ... done in 30.43 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  10357582\n",
      "                          * data length       =  76422\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk14 ... done in 8.92 second(s)\n",
      "chunk preprocessing ... done in 114.89 second(s)\n",
      "generating stop steps sequences ... done in 14.89 second(s)\n",
      "extracting preprocessed numpy array data ... done in 46.88 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  10164811\n",
      "                          * data length       =  76214\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk15 ... done in 13.90 second(s)\n",
      "chunk preprocessing ... done in 117.44 second(s)\n",
      "generating stop steps sequences ... done in 14.80 second(s)\n",
      "extracting preprocessed numpy array data ... done in 28.12 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9957175\n",
      "                          * data length       =  75519\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk16 ... done in 9.42 second(s)\n",
      "chunk preprocessing ... done in 115.08 second(s)\n",
      "generating stop steps sequences ... done in 15.08 second(s)\n",
      "extracting preprocessed numpy array data ... done in 22.51 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  10067147\n",
      "                          * data length       =  75088\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk17 ... done in 7.74 second(s)\n",
      "chunk preprocessing ... done in 112.49 second(s)\n",
      "generating stop steps sequences ... done in 14.49 second(s)\n",
      "extracting preprocessed numpy array data ... done in 38.32 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9706633\n",
      "                          * data length       =  73863\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk18 ... done in 10.78 second(s)\n",
      "chunk preprocessing ... done in 114.00 second(s)\n",
      "generating stop steps sequences ... done in 14.36 second(s)\n",
      "extracting preprocessed numpy array data ... done in 27.77 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9757776\n",
      "                          * data length       =  73930\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk19 ... done in 10.78 second(s)\n",
      "chunk preprocessing ... done in 112.30 second(s)\n",
      "generating stop steps sequences ... done in 14.66 second(s)\n",
      "extracting preprocessed numpy array data ... done in 25.37 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  10028543\n",
      "                          * data length       =  74925\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk2 ... done in 7.25 second(s)\n",
      "chunk preprocessing ... done in 109.99 second(s)\n",
      "generating stop steps sequences ... done in 14.13 second(s)\n",
      "extracting preprocessed numpy array data ... done in 26.37 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9735145\n",
      "                          * data length       =  71734\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk20 ... done in 7.58 second(s)\n",
      "chunk preprocessing ... done in 111.83 second(s)\n",
      "generating stop steps sequences ... done in 14.33 second(s)\n",
      "extracting preprocessed numpy array data ... done in 25.18 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9891317\n",
      "                          * data length       =  75561\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk21 ... done in 9.04 second(s)\n",
      "chunk preprocessing ... done in 108.35 second(s)\n",
      "generating stop steps sequences ... done in 13.87 second(s)\n",
      "extracting preprocessed numpy array data ... done in 20.19 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9638191\n",
      "                          * data length       =  72883\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk22 ... done in 7.18 second(s)\n",
      "chunk preprocessing ... done in 98.21 second(s)\n",
      "generating stop steps sequences ... done in 12.06 second(s)\n",
      "extracting preprocessed numpy array data ... done in 21.84 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  8535543\n",
      "                          * data length       =  65449\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk23 ... done in 7.71 second(s)\n",
      "chunk preprocessing ... done in 85.76 second(s)\n",
      "generating stop steps sequences ... done in 10.77 second(s)\n",
      "extracting preprocessed numpy array data ... done in 19.76 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  7231243\n",
      "                          * data length       =  55307\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk24 ... done in 2.54 second(s)\n",
      "chunk preprocessing ... done in 32.67 second(s)\n",
      "generating stop steps sequences ... done in 4.25 second(s)\n",
      "extracting preprocessed numpy array data ... done in 4.31 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  2708572\n",
      "                          * data length       =  21020\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk3 ... done in 7.17 second(s)\n",
      "chunk preprocessing ... done in 105.86 second(s)\n",
      "generating stop steps sequences ... done in 12.94 second(s)\n",
      "extracting preprocessed numpy array data ... done in 159.61 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9341208\n",
      "                          * data length       =  69075\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk4 ... done in 6.08 second(s)\n",
      "chunk preprocessing ... done in 35.19 second(s)\n",
      "generating stop steps sequences ... done in 3.72 second(s)\n",
      "extracting preprocessed numpy array data ... done in 4.46 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  2114020\n",
      "                          * data length       =  24855\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk5 ... done in 3.49 second(s)\n",
      "chunk preprocessing ... done in 37.04 second(s)\n",
      "generating stop steps sequences ... done in 3.43 second(s)\n",
      "extracting preprocessed numpy array data ... done in 11.56 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  2229448\n",
      "                          * data length       =  26001\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk6 ... done in 4.23 second(s)\n",
      "chunk preprocessing ... done in 45.50 second(s)\n",
      "generating stop steps sequences ... done in 4.30 second(s)\n",
      "extracting preprocessed numpy array data ... done in 6.59 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  2581252\n",
      "                          * data length       =  29212\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk7 ... done in 6.93 second(s)\n",
      "chunk preprocessing ... done in 44.13 second(s)\n",
      "generating stop steps sequences ... done in 3.91 second(s)\n",
      "extracting preprocessed numpy array data ... done in 9.97 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  2494012\n",
      "                          * data length       =  28030\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk8 ... done in 3.62 second(s)\n",
      "chunk preprocessing ... done in 46.26 second(s)\n",
      "generating stop steps sequences ... done in 4.64 second(s)\n",
      "extracting preprocessed numpy array data ... done in 5.65 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  2612022\n",
      "                          * data length       =  29493\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk9 ... done in 3.28 second(s)\n",
      "chunk preprocessing ... done in 41.71 second(s)\n",
      "generating stop steps sequences ... done in 3.89 second(s)\n",
      "extracting preprocessed numpy array data ... done in 5.36 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  2457450\n",
      "                          * data length       =  27971\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Final report : \n",
      "               * total records sum =  177422114\n",
      "               * data length       =  1393862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import gc\n",
    "import random\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "filenames = os.listdir('../dataPreprocessing/data/fullpick')[1:]\n",
    "filenames = ['../dataPreprocessing/data/fullpick/'+ filename for filename in filenames]\n",
    "names = [name[:-4] for name in os.listdir('../dataPreprocessing/data/fullpick') ][1:]\n",
    "\n",
    "print('---------------------------ETL------------------------------', end='\\n')\n",
    "print('\\n\\n',end='')\n",
    "\n",
    "data_length_sum = 0\n",
    "features_records_sum = 0\n",
    "maxlength_exeeded = 0\n",
    "\n",
    "for c, filename in enumerate(filenames):\n",
    "    print('loading chunk ',names[c], end=' ... ')\n",
    "    start = time()\n",
    "    #load chunk\n",
    "    fullpick = pd.read_csv(filename)\n",
    "    stop = time()\n",
    "    print('done in {:.2f} second(s)'.format(stop-start))\n",
    "\n",
    "    print('chunk preprocessing', end=' ... ')\n",
    "    #convert str to datetime\n",
    "    start = time()\n",
    "    fullpick.datetime = fullpick.datetime.apply(lambda x : datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    #datetime features : \n",
    "    fullpick['hour'] = fullpick.datetime.dt.hour\n",
    "    fullpick['day'] = fullpick.datetime.dt.day\n",
    "    fullpick['month'] = fullpick.datetime.dt.month\n",
    "    fullpick['quarter'] = fullpick.datetime.dt.quarter\n",
    "    fullpick['dayofweek'] = fullpick.datetime.dt.dayofweek\n",
    "    fullpick['service_class'] = fullpick['datetime'].dt.weekday.apply(lambda x : 1 if x in [5,6] else 0)\n",
    "\n",
    "    #map true line_id\n",
    "    fullpick['correct_line_id'] = fullpick.sequence_id.map(correct_lineid)   #affect target (True line_id)\n",
    "    fullpick['correct_line_id'] = fullpick.correct_line_id.map(target_encoder)    #target label encoder\n",
    "\n",
    "    #scale data between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    fullpick.vehicle_id =  scaler.fit_transform(fullpick[['vehicle_id']])\n",
    "    fullpick.line_id =  scaler.fit_transform(fullpick[['line_id']])\n",
    "    fullpick.latitude =  scaler.fit_transform(fullpick[['latitude']])\n",
    "    fullpick.longitude =  scaler.fit_transform(fullpick[['longitude']])\n",
    "    fullpick.direction =  scaler.fit_transform(fullpick[['direction']])\n",
    "    fullpick.vehicle_type =  scaler.fit_transform(fullpick[['vehicle_type']])\n",
    "    fullpick.hour =  scaler.fit_transform(fullpick[['hour']])\n",
    "    fullpick.day =  scaler.fit_transform(fullpick[['day']])\n",
    "    fullpick.month =  scaler.fit_transform(fullpick[['month']])\n",
    "    fullpick.quarter =  scaler.fit_transform(fullpick[['quarter']])\n",
    "    fullpick.dayofweek =  scaler.fit_transform(fullpick[['dayofweek']])\n",
    "    fullpick.service_class =  scaler.fit_transform(fullpick[['service_class']])\n",
    "\n",
    "    #grouping sequences\n",
    "    grouping_dict = {'sequence_id':'first', 'station_id':'first', 'vehicle_id':'first', 'line_id':'first', 'correct_line_id':'first', 'vehicle_type':'first',\n",
    "                     'latitude':list, 'longitude':list, 'direction':list, 'hour':list, 'day':list, 'month':list, 'quarter':list,\n",
    "                     'dayofweek':list, 'service_class':list}\n",
    "    fullpick = fullpick.set_index('datetime').groupby(['sequence_id','station_id'], as_index=False).agg(grouping_dict).reset_index(drop = True)\n",
    "    stop = time()\n",
    "    print('done in {:.2f} second(s)'.format(stop-start))\n",
    "\n",
    "    print('generating stop steps sequences', end=' ... ')\n",
    "    #generating X_train and y_train time step squences\n",
    "    start = time()\n",
    "    n = fullpick.shape[0]\n",
    "    features = []\n",
    "    labels = []\n",
    "    sequence_timestep = {}\n",
    "    line_id_sequence_map = {}\n",
    "    \n",
    "    for sequence_id, station_id, vehicle_id, line_id, correct_line_id, vehicle_type, \\\n",
    "        latitude, longitude, direction, hour, day, month, quarter, dayofweek, service_class in fullpick.values :\n",
    "        n = len(latitude)\n",
    "        samples = []\n",
    "        sample = np.zeros(12)\n",
    "        for i in range(n):\n",
    "            sample[0] = vehicle_id\n",
    "            sample[1] = line_id\n",
    "            sample[2] = latitude[i]\n",
    "            sample[3] = longitude[i]\n",
    "            sample[4] = direction[i]\n",
    "            sample[5] = vehicle_type\n",
    "            sample[6] = hour[i]\n",
    "            sample[7] = day[i]\n",
    "            sample[8] = month[i]\n",
    "            sample[9] = quarter[i]\n",
    "            sample[10] = dayofweek[i]\n",
    "            sample[11] = service_class[i]\n",
    "            samples.append(sample)\n",
    "        if sequence_id in sequence_timestep : \n",
    "            sequence_timestep[sequence_id].append(samples)       \n",
    "        else:\n",
    "            sequence_timestep[sequence_id] = [samples]\n",
    "        labels.append(np.array(correct_line_id))\n",
    "\n",
    "    #add history of past time steps to sequences\n",
    "    for sequence in sequence_timestep.keys() :    #loop on each sequence\n",
    "        history = []         #history keeps 25% of data for each past time step (data is selected randomly)\n",
    "        for i, timestep in enumerate(sequence_timestep[sequence]) :     #loop on each sequence time step\n",
    "            if i != 0 :\n",
    "                sequence_timestep[sequence][i] = history + sequence_timestep[sequence][i]    #add history list at the start of the current timestep\n",
    "                \n",
    "            #if sequence_timestep dims exeeded 200, select 200 random sample \n",
    "            if len(sequence_timestep[sequence][i]) > 200 :\n",
    "                sequence_timestep[sequence][i] = random.sample(sequence_timestep[sequence][i], 200)\n",
    "                maxlength_exeeded += 1\n",
    "                \n",
    "            #add 25% of the current time_step at the end of history list \n",
    "            n = len(timestep)\n",
    "            k = round(n*0.25)\n",
    "            history += random.sample(timestep, k)\n",
    "\n",
    "    #convert sequences to numpy arrays\n",
    "    for sequence in sequence_timestep.keys() :\n",
    "        for i in range(len(sequence_timestep[sequence])) :\n",
    "            sequence_timestep[sequence][i] = np.array(sequence_timestep[sequence][i])\n",
    "        sequence_timestep[sequence] = np.array(sequence_timestep[sequence], object)\n",
    "\n",
    "    #generate X_train numpy array \n",
    "    features = []\n",
    "    for path in sequence_timestep.values():\n",
    "        for sequence in path : \n",
    "            features.append(sequence)\n",
    "\n",
    "    #convert X_train and y_train to numpy arrays\n",
    "    features = np.array(features, dtype=object)\n",
    "    labels = np.array(labels).reshape(-1,1)\n",
    "    stop = time()\n",
    "    print('done in {:.2f} second(s)'.format(stop-start))\n",
    "\n",
    "    print('extracting preprocessed numpy array data', end=' ... ')\n",
    "    #export data ready to consume by TF models\n",
    "    start = time()\n",
    "    np.save('../dataPreprocessing/data/Fullpick_ETL/features/'+names[c]+'.npy', features)\n",
    "    np.save('../dataPreprocessing/data/Fullpick_ETL/labels/'+names[c]+'.npy', labels)\n",
    "    stop = time()\n",
    "    print('done in {:.2f} second(s)'.format(stop-start))\n",
    "\n",
    "    #sequences and records data stats\n",
    "    tot_recs = 0\n",
    "    for i in range(features.shape[0]):\n",
    "        tot_recs += features[i].shape[0]\n",
    "    features_records_sum += tot_recs\n",
    "    data_length_sum += labels.shape[0]\n",
    "\n",
    "    print('chunk statistics report : ')\n",
    "    print('                          * total records sum = ',tot_recs)\n",
    "    print('                          * data length       = ',labels.shape[0])\n",
    "\n",
    "    #delete temp objects\n",
    "    print('delete temp objects', end=' ... ')\n",
    "    del features\n",
    "    del labels\n",
    "    del samples\n",
    "    del sample\n",
    "    del fullpick\n",
    "    gc.collect()\n",
    "    print('done', end='\\n\\n')\n",
    "\n",
    "    print('------------------------------------------------------------------------', end='\\n\\n')\n",
    "\n",
    "print('Final report : ')\n",
    "print('               * total records sum = ',features_records_sum)\n",
    "print('               * data length       = ',data_length_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "japanese-crown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246753"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlength_exeeded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-simulation",
   "metadata": {},
   "source": [
    "### Extract a stratified (with respect to line_id) sequence sample (10% of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "institutional-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "features_path = '../dataPreprocessing/data/Fullpick_ETL/features/'\n",
    "labels_path = '../dataPreprocessing/data/Fullpick_ETL/labels/'\n",
    "filenames = sorted(os.listdir('../dataPreprocessing/data/Fullpick_ETL/features'), key = lambda x : int(re.findall(r'\\d+',x)[0]) )\n",
    "\n",
    "features_names = [ features_path+filename for filename in filenames ]\n",
    "label_names = [ labels_path+filename for filename in filenames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "disabled-slave",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [10:47, 26.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "for feature_filename, label_filename in tqdm(zip(features_names, label_names)):\n",
    "\n",
    "    #load data\n",
    "    features = np.load(feature_filename, allow_pickle=True)\n",
    "    labels = np.load(label_filename, allow_pickle=False)\n",
    "    \n",
    "    #startified data sampling\n",
    "    _, stratified_features, _, stratified_labels  = train_test_split(features, labels, test_size=0.1, stratify=labels)\n",
    "\n",
    "    #save startified samples\n",
    "    np.save(feature_filename.replace('features','startified_sample/features'), stratified_features)\n",
    "    np.save(label_filename.replace('labels','startified_sample/labels'), stratified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-duration",
   "metadata": {},
   "source": [
    "### Join all startified samples into one train/test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "endangered-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "features_path = '../dataPreprocessing/data/Fullpick_ETL/startified_sample/features/'\n",
    "labels_path = '../dataPreprocessing/data/Fullpick_ETL/startified_sample/labels/'\n",
    "\n",
    "filenames = sorted(os.listdir('../dataPreprocessing/data/Fullpick_ETL/startified_sample/features'), key = lambda x : int(re.findall(r'\\d+',x)[0]) )\n",
    "\n",
    "features_names = [ features_path+filename for filename in filenames ]\n",
    "label_names = [ labels_path+filename for filename in filenames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "handed-italy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:36,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample length :  139397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "first  = True\n",
    "for feature_filename, label_filename in tqdm(zip(features_names, label_names)):\n",
    "    #load data\n",
    "    if first :\n",
    "        first  = False\n",
    "        data   = np.load(feature_filename, allow_pickle=True)\n",
    "        target = np.load(label_filename, allow_pickle=False)\n",
    "    else : \n",
    "        data = np.append(data, np.load(feature_filename, allow_pickle=True))\n",
    "        target = np.append(target, np.load(label_filename, allow_pickle=False))\n",
    "\n",
    "print('sample length : ', len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "roman-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_path = '../dataPreprocessing/data/Fullpick_ETL/startified_sample/merged_sample/'\n",
    "\n",
    "np.save(extraction_path+'features', data)\n",
    "np.save(extraction_path+'labels', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-drill",
   "metadata": {},
   "source": [
    "### Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "federal-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../dataPreprocessing/data/Fullpick_ETL/startified_sample/merged_sample/features.npy', allow_pickle=True)\n",
    "target = np.load('../dataPreprocessing/data/Fullpick_ETL/startified_sample/merged_sample/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[:90000]\n",
    "y_train = target[:90000]\n",
    "\n",
    "X_val = data[90000:95000]\n",
    "y_val = target[90000:95000]\n",
    "\n",
    "X_test = data[95000:]\n",
    "y_test = target[95000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture keras API\n",
    "\n",
    "def generate_model(hp=None):\n",
    "    #default hyper parameters\n",
    "    num_stacked_dense = 1\n",
    "    droput_rate       = 0.2\n",
    "    gru_units         = 128\n",
    "    dense_units       = 128\n",
    "    learning_rate     = 0.002\n",
    "    num_stacked_gru   = 0\n",
    "    dense_activation  = 'relu'\n",
    "    \n",
    "    if hp:\n",
    "        num_stacked_dense = hp.Choice('num_stacked_dense', values=[1, 2, 3, 4])\n",
    "        num_stacked_gru   = hp.Choice('num_stacked_gru', values=[0, 1])\n",
    "        dense_activation  = hp.Choice('dense_activation', values=['relu', 'elu', 'tanh']) \n",
    "        droput_rate       = hp.Float('droput_rate', min_value=0.1 , max_value=0.5)\n",
    "        learning_rate     = hp.Float('learning_rate', min_value=0.001, max_value=0.01)\n",
    "        gru_units         = hp.Int('gru_units', min_value=64, max_value=256, step=32)\n",
    "        dense_units       = hp.Int('dense_units', min_value=128, max_value=320, step=32)\n",
    "\n",
    "    inputs = Input(shape = (None,12), ragged=True)\n",
    "\n",
    "    #GRU layers Block\n",
    "    if num_stacked_gru > 0 :\n",
    "        x = layers.GRU(units=gru_units, activation='tanh' , input_shape=(-1, None, 12), return_sequences=True )(inputs)\n",
    "        for i in range(num_stacked_gru):\n",
    "            if i+1 == num_stacked_gru:\n",
    "                x = GRU(units = gru_units, activation='tanh' , return_sequences = False )(inputs)\n",
    "            else:\n",
    "                x = layers.GRU(units=gru_units, activation='tanh', return_sequences=True )(x)\n",
    "    else : \n",
    "        x = GRU(units = gru_units, activation='tanh' , input_shape = (-1, None, 12), return_sequences = False )(inputs)\n",
    "    \n",
    "    #Dense layers block\n",
    "    for i in range(num_stacked_dense):\n",
    "        x = layers.Dense(dense_units, activation=dense_activation)(x)\n",
    "        x = layers.Dropout(droput_rate)(x)\n",
    "    #output layer\n",
    "    out = layers.Dense(43, activation='softmax')(x)\n",
    "    \n",
    "    #build model\n",
    "    model = Model(inputs, out)\n",
    "    #model compilation\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    loss = SparseCategoricalCrossentropy(from_logits = True)\n",
    "    metric = SparseCategoricalAccuracy()\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metric)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.ragged.constant(X_train, dtype=tf.float32)\n",
    "y_train = tf.constant(y_train, dtype=tf.int32)\n",
    "\n",
    "X_val  = tf.ragged.constant(X_val, dtype=tf.float32)\n",
    "y_val  = tf.constant(y_val, dtype=tf.int32)\n",
    "\n",
    "X_test  = tf.ragged.constant(X_test, dtype=tf.float32)\n",
    "y_test  = tf.constant(y_test, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow data pipeline\n",
    "ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_data = ds.batch(1000).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = BayesianOptimization(\n",
    "    generate_model,\n",
    "    objective = kerastuner.Objective(\"val_sparse_categorical_accuracy\", direction=\"max\"),\n",
    "    max_trials = 20,\n",
    "    directory = 'log',\n",
    "    project_name = 'path_prediction',\n",
    "    overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_data, validation_data=(X_val, y_val), epochs=30, verbose=True,\n",
    "             callbacks=[EarlyStopping(restore_best_weights=True, patience=3, monitor='val_root_mean_squared_error')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('test loss : {:.2f}    test accuracy : {:.2f}% '.format(loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model\n",
    "path_model.save_weights('./checkpoint/path_many_to_one.cpkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-drove",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-assessment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
