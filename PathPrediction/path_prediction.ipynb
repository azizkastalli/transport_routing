{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fourth-grammar",
   "metadata": {},
   "source": [
    "## Correct path prediction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chubby-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "interracial-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "from tensorflow .data import Dataset\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nervous-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "japanese-puppy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1950296 entries, 0 to 1950295\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   vehicle_id     int64  \n",
      " 1   line_id        int64  \n",
      " 2   latitude       float64\n",
      " 3   longitude      float64\n",
      " 4   datetime       object \n",
      " 5   station_id     int64  \n",
      " 6   vehicle_type   float64\n",
      " 7   sequence_id    object \n",
      " 8   order          int64  \n",
      " 9   line_label     object \n",
      " 10  datetime_diff  float64\n",
      " 11  outlier        int64  \n",
      "dtypes: float64(4), int64(5), object(3)\n",
      "memory usage: 178.6+ MB\n"
     ]
    }
   ],
   "source": [
    "picktime = pd.read_csv('../dataPreprocessing/data/gps_clean.csv', low_memory=False)\n",
    "picktime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interim-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = dict( (target,code) for code, target in enumerate(picktime.line_id.unique()) )\n",
    "target_decoder = dict( (code, target) for target, code in target_encoder.items() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "skilled-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_targets = picktime.line_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naughty-notice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "anticipated-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_lineid = picktime[['sequence_id','line_id']].groupby('sequence_id').first().line_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-missile",
   "metadata": {},
   "source": [
    "### DATA ETL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "solved-lindsay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------ETL------------------------------\n",
      "\n",
      "\n",
      "loading chunk  fullpick_chunk1 ... done in 15.24 second(s)\n",
      "chunk preprocessing ... done in 294.01 second(s)\n",
      "generating stop steps sequences ... done in 65.97 second(s)\n",
      "extracting preprocessed numpy array data ... done in 63.42 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  14157000\n",
      "                          * data length       =  70785\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk10 ... done in 33.53 second(s)\n",
      "chunk preprocessing ... done in 206.15 second(s)\n",
      "generating stop steps sequences ... done in 48.55 second(s)\n",
      "extracting preprocessed numpy array data ... done in 9.70 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  9943400\n",
      "                          * data length       =  49717\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk11 ... done in 34.37 second(s)\n",
      "chunk preprocessing ... done in 230.74 second(s)\n",
      "generating stop steps sequences ... done in 29.29 second(s)\n",
      "extracting preprocessed numpy array data ... done in 11.55 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  15251800\n",
      "                          * data length       =  76259\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk12 ... done in 5.93 second(s)\n",
      "chunk preprocessing ... done in 92.29 second(s)\n",
      "generating stop steps sequences ... done in 22.07 second(s)\n",
      "extracting preprocessed numpy array data ... done in 17.31 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  14909800\n",
      "                          * data length       =  74549\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk13 ... done in 7.65 second(s)\n",
      "chunk preprocessing ... done in 93.78 second(s)\n",
      "generating stop steps sequences ... done in 23.21 second(s)\n",
      "extracting preprocessed numpy array data ... done in 17.27 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  15284400\n",
      "                          * data length       =  76422\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk14 ... done in 6.21 second(s)\n",
      "chunk preprocessing ... done in 93.45 second(s)\n",
      "generating stop steps sequences ... done in 23.23 second(s)\n",
      "extracting preprocessed numpy array data ... done in 17.87 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  15242800\n",
      "                          * data length       =  76214\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk15 ... done in 10.58 second(s)\n",
      "chunk preprocessing ... done in 93.48 second(s)\n",
      "generating stop steps sequences ... done in 23.08 second(s)\n",
      "extracting preprocessed numpy array data ... done in 107.90 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  15103800\n",
      "                          * data length       =  75519\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk16 ... done in 12.29 second(s)\n",
      "chunk preprocessing ... done in 96.27 second(s)\n",
      "generating stop steps sequences ... done in 23.85 second(s)\n",
      "extracting preprocessed numpy array data ... done in 44.31 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  15017600\n",
      "                          * data length       =  75088\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk17 ... done in 11.57 second(s)\n",
      "chunk preprocessing ... done in 93.20 second(s)\n",
      "generating stop steps sequences ... done in 22.23 second(s)\n",
      "extracting preprocessed numpy array data ... done in 15.46 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  14772600\n",
      "                          * data length       =  73863\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk18 ... done in 10.65 second(s)\n",
      "chunk preprocessing ... done in 91.15 second(s)\n",
      "generating stop steps sequences ... done in 22.14 second(s)\n",
      "extracting preprocessed numpy array data ... done in 26.73 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  14786000\n",
      "                          * data length       =  73930\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk19 ... done in 6.78 second(s)\n",
      "chunk preprocessing ... done in 93.83 second(s)\n",
      "generating stop steps sequences ... done in 23.51 second(s)\n",
      "extracting preprocessed numpy array data ... done in 9.15 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  14985000\n",
      "                          * data length       =  74925\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk2 ... done in 7.95 second(s)\n",
      "chunk preprocessing ... done in 91.06 second(s)\n",
      "generating stop steps sequences ... done in 22.42 second(s)\n",
      "extracting preprocessed numpy array data ... done in 11.97 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  14346800\n",
      "                          * data length       =  71734\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk20 ... done in 6.51 second(s)\n",
      "chunk preprocessing ... done in 90.40 second(s)\n",
      "generating stop steps sequences ... done in 22.69 second(s)\n",
      "extracting preprocessed numpy array data ... done in 15.01 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  15112200\n",
      "                          * data length       =  75561\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk21 ... done in 6.34 second(s)\n",
      "chunk preprocessing ... done in 88.21 second(s)\n",
      "generating stop steps sequences ... done in 22.90 second(s)\n",
      "extracting preprocessed numpy array data ... done in 10.59 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  14576600\n",
      "                          * data length       =  72883\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk22 ... done in 5.53 second(s)\n",
      "chunk preprocessing ... done in 78.19 second(s)\n",
      "generating stop steps sequences ... done in 20.75 second(s)\n",
      "extracting preprocessed numpy array data ... done in 29.47 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  13089800\n",
      "                          * data length       =  65449\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk23 ... done in 10.17 second(s)\n",
      "chunk preprocessing ... done in 70.40 second(s)\n",
      "generating stop steps sequences ... done in 16.85 second(s)\n",
      "extracting preprocessed numpy array data ... done in 9.21 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  11061400\n",
      "                          * data length       =  55307\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk24 ... done in 2.08 second(s)\n",
      "chunk preprocessing ... done in 26.61 second(s)\n",
      "generating stop steps sequences ... done in 6.77 second(s)\n",
      "extracting preprocessed numpy array data ... done in 2.24 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  4204000\n",
      "                          * data length       =  21020\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk3 ... done in 5.43 second(s)\n",
      "chunk preprocessing ... done in 86.37 second(s)\n",
      "generating stop steps sequences ... done in 21.66 second(s)\n",
      "extracting preprocessed numpy array data ... done in 13.31 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  13815000\n",
      "                          * data length       =  69075\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk4 ... done in 2.07 second(s)\n",
      "chunk preprocessing ... done in 27.19 second(s)\n",
      "generating stop steps sequences ... done in 8.81 second(s)\n",
      "extracting preprocessed numpy array data ... done in 3.74 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  4971000\n",
      "                          * data length       =  24855\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk5 ... done in 1.80 second(s)\n",
      "chunk preprocessing ... done in 28.91 second(s)\n",
      "generating stop steps sequences ... done in 8.84 second(s)\n",
      "extracting preprocessed numpy array data ... done in 5.75 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  5200200\n",
      "                          * data length       =  26001\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk6 ... done in 2.29 second(s)\n",
      "chunk preprocessing ... done in 33.32 second(s)\n",
      "generating stop steps sequences ... done in 9.92 second(s)\n",
      "extracting preprocessed numpy array data ... done in 3.96 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  5842400\n",
      "                          * data length       =  29212\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk7 ... done in 2.01 second(s)\n",
      "chunk preprocessing ... done in 32.42 second(s)\n",
      "generating stop steps sequences ... done in 9.71 second(s)\n",
      "extracting preprocessed numpy array data ... done in 2.99 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  5606000\n",
      "                          * data length       =  28030\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk8 ... done in 3.31 second(s)\n",
      "chunk preprocessing ... done in 33.53 second(s)\n",
      "generating stop steps sequences ... done in 9.88 second(s)\n",
      "extracting preprocessed numpy array data ... done in 5.78 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  5898600\n",
      "                          * data length       =  29493\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "loading chunk  fullpick_chunk9 ... done in 2.15 second(s)\n",
      "chunk preprocessing ... done in 31.92 second(s)\n",
      "generating stop steps sequences ... done in 9.42 second(s)\n",
      "extracting preprocessed numpy array data ... done in 6.47 second(s)\n",
      "chunk statistics report : \n",
      "                          * total records sum =  5594200\n",
      "                          * data length       =  27971\n",
      "delete temp objects ... done\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Final report : \n",
      "               * total records sum =  278772400\n",
      "               * data length       =  1393862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import gc\n",
    "import random\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "filenames = os.listdir('../dataPreprocessing/data/fullpick')[1:]\n",
    "filenames = ['../dataPreprocessing/data/fullpick/'+ filename for filename in filenames]\n",
    "names = [name[:-4] for name in os.listdir('../dataPreprocessing/data/fullpick') ][1:]\n",
    "\n",
    "print('---------------------------ETL------------------------------', end='\\n')\n",
    "print('\\n\\n',end='')\n",
    "\n",
    "data_length_sum      = 0\n",
    "features_records_sum = 0\n",
    "maxlength_exeeded    = 0\n",
    "sequence_fixedSize   = 200\n",
    "history_track_rate   = 0.25\n",
    "\n",
    "for c, filename in enumerate(filenames):\n",
    "    print('loading chunk ',names[c], end=' ... ')\n",
    "    start = time()\n",
    "    #load chunk\n",
    "    fullpick = pd.read_csv(filename)\n",
    "    stop = time()\n",
    "    print('done in {:.2f} second(s)'.format(stop-start))\n",
    "\n",
    "    print('chunk preprocessing', end=' ... ')\n",
    "    #convert str to datetime\n",
    "    start = time()\n",
    "    fullpick.datetime = fullpick.datetime.apply(lambda x : datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    #timestamp feature :\n",
    "    fullpick['timestamp'] = fullpick.datetime.apply(lambda x : x.timestamp())\n",
    "    \n",
    "\n",
    "    #map true line_id\n",
    "    fullpick['correct_line_id'] = fullpick.sequence_id.map(correct_lineid)   #affect target (True line_id)\n",
    "    fullpick['correct_line_id'] = fullpick.correct_line_id.map(target_encoder)    #target label encoder\n",
    "\n",
    "    #scale data between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    fullpick.line_id       =  scaler.fit_transform(fullpick[['line_id']])\n",
    "    fullpick.latitude      =  scaler.fit_transform(fullpick[['latitude']])\n",
    "    fullpick.longitude     =  scaler.fit_transform(fullpick[['longitude']])\n",
    "    fullpick.direction     =  scaler.fit_transform(fullpick[['direction']])\n",
    "    fullpick.vehicle_type  =  scaler.fit_transform(fullpick[['vehicle_type']])\n",
    "    fullpick.timestamp     =  scaler.fit_transform(fullpick[['timestamp']])\n",
    "\n",
    "\n",
    "    #grouping sequences\n",
    "    grouping_dict = {'sequence_id':'first', 'station_id':'first', 'line_id':'first', 'correct_line_id':'first', 'vehicle_type':'first',\n",
    "                     'latitude':list, 'longitude':list, 'direction':list, 'timestamp':list }\n",
    "    fullpick = fullpick.set_index('datetime').groupby(['sequence_id','station_id'], as_index=False).agg(grouping_dict).reset_index(drop = True)\n",
    "    stop = time()\n",
    "    print('done in {:.2f} second(s)'.format(stop-start))\n",
    "\n",
    "    print('generating stop steps sequences', end=' ... ')\n",
    "    #generating X_train and y_train time step squences\n",
    "    start = time()\n",
    "    n     = fullpick.shape[0]\n",
    "    features = []\n",
    "    labels   = []\n",
    "    sequence_timestep    = {}\n",
    "    line_id_sequence_map = {}\n",
    "    \n",
    "    for sequence_id, station_id, line_id, correct_line_id, vehicle_type, latitude, longitude, direction, timestamp in fullpick.values :\n",
    "        n = len(latitude)\n",
    "        samples = []\n",
    "        sample  = np.zeros(6)\n",
    "        for i in range(n):\n",
    "            sample[0]  = latitude[i]\n",
    "            sample[1]  = longitude[i]\n",
    "            sample[2]  = direction[i]\n",
    "            sample[3]  = timestamp[i]\n",
    "            sample[4]  = vehicle_type\n",
    "            sample[5]  = line_id\n",
    "            samples.append(sample)\n",
    "            \n",
    "        if sequence_id in sequence_timestep : \n",
    "            sequence_timestep[sequence_id].append(samples)       \n",
    "        else:\n",
    "            sequence_timestep[sequence_id] = [samples]\n",
    "        \n",
    "        labels.append(np.array(correct_line_id))\n",
    "\n",
    "    #add history of past time steps to sequences\n",
    "    for sequence in sequence_timestep.keys() :    #loop on each sequence\n",
    "        history = []         #history keeps 25% of data for each past time step (data is selected randomly)\n",
    "        for i, timestep in enumerate(sequence_timestep[sequence]) :     #loop on each sequence time step\n",
    "            if i != 0 :\n",
    "                sequence_timestep[sequence][i] = history + sequence_timestep[sequence][i]    #add history list at the start of the current timestep\n",
    "            \n",
    "            #if sequence_timestep is less than 200 steps, add zero padding at the end of the sequence\n",
    "            if len(sequence_timestep[sequence][i]) < sequence_fixedSize :\n",
    "                nb_zeros = sequence_fixedSize - len(sequence_timestep[sequence][i])\n",
    "                sequence_timestep[sequence][i] += np.zeros((nb_zeros,6)).tolist()\n",
    "            \n",
    "            #if sequence_timestep dims exeeded 200, select 200 random sample \n",
    "            if len(sequence_timestep[sequence][i]) > sequence_fixedSize :\n",
    "                sequence_timestep[sequence][i] = random.sample(sequence_timestep[sequence][i], sequence_fixedSize)\n",
    "                maxlength_exeeded += 1\n",
    "                \n",
    "            #add 25% of the current time_step at the end of history list \n",
    "            n = len(timestep)\n",
    "            k = round(n*history_track_rate)\n",
    "            history += random.sample(timestep, k)\n",
    "\n",
    "    #convert sequences to numpy arrays\n",
    "    for sequence in sequence_timestep.keys() :\n",
    "        for i in range(len(sequence_timestep[sequence])) :\n",
    "            sequence_timestep[sequence][i] = np.array(sequence_timestep[sequence][i])\n",
    "        sequence_timestep[sequence] = np.array(sequence_timestep[sequence], dtype=np.float)\n",
    "\n",
    "    #generate X_train numpy array \n",
    "    features = []\n",
    "    for path in sequence_timestep.values():\n",
    "        for sequence in path : \n",
    "            features.append(sequence)\n",
    "\n",
    "    #convert X_train and y_train to numpy arrays\n",
    "    features = np.array(features, dtype=np.float)\n",
    "    labels = np.array(labels).reshape(-1,1)\n",
    "    stop = time()\n",
    "    print('done in {:.2f} second(s)'.format(stop-start))\n",
    "\n",
    "    print('extracting preprocessed numpy array data', end=' ... ')\n",
    "    #export data ready to consume by TF models\n",
    "    start = time()\n",
    "    np.save('../dataPreprocessing/data/Fullpick_ETL/features/'+names[c]+'.npy', features)\n",
    "    np.save('../dataPreprocessing/data/Fullpick_ETL/labels/'+names[c]+'.npy', labels)\n",
    "    stop = time()\n",
    "    print('done in {:.2f} second(s)'.format(stop-start))\n",
    "\n",
    "    #sequences and records data stats\n",
    "    tot_recs = 0\n",
    "    for i in range(features.shape[0]):\n",
    "        tot_recs += features[i].shape[0]\n",
    "    features_records_sum += tot_recs\n",
    "    data_length_sum += labels.shape[0]\n",
    "\n",
    "    print('chunk statistics report : ')\n",
    "    print('                          * total records sum = ',tot_recs)\n",
    "    print('                          * data length       = ',labels.shape[0])\n",
    "\n",
    "    #delete temp objects\n",
    "    print('delete temp objects', end=' ... ')\n",
    "    del features\n",
    "    del labels\n",
    "    del samples\n",
    "    del sample\n",
    "    del fullpick\n",
    "    gc.collect()\n",
    "    print('done', end='\\n\\n')\n",
    "\n",
    "    print('------------------------------------------------------------------------', end='\\n\\n')\n",
    "\n",
    "print('Final report : ')\n",
    "print('               * total records sum = ',features_records_sum)\n",
    "print('               * data length       = ',data_length_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-kidney",
   "metadata": {},
   "source": [
    "### Extract a stratified (with respect to line_id) sequence sample (10% of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "obvious-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "features_path = '../dataPreprocessing/data/Fullpick_ETL/features/'\n",
    "labels_path = '../dataPreprocessing/data/Fullpick_ETL/labels/'\n",
    "filenames = sorted(os.listdir('../dataPreprocessing/data/Fullpick_ETL/features'), key = lambda x : int(re.findall(r'\\d+',x)[0]) )\n",
    "\n",
    "features_names = [ features_path+filename for filename in filenames ]\n",
    "label_names = [ labels_path+filename for filename in filenames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "consolidated-breakdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [06:31, 16.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "for feature_filename, label_filename in tqdm(zip(features_names, label_names)):\n",
    "\n",
    "    #load data\n",
    "    features = np.load(feature_filename, allow_pickle=True)\n",
    "    labels = np.load(label_filename, allow_pickle=False)\n",
    "    \n",
    "    #startified data sampling\n",
    "    _, stratified_features, _, stratified_labels  = train_test_split(features, labels, test_size=0.1, stratify=labels)\n",
    "\n",
    "    #save startified samples\n",
    "    np.save(feature_filename.replace('features','startified_sample/features'), stratified_features)\n",
    "    np.save(label_filename.replace('labels','startified_sample/labels'), stratified_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "super-thought",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7079, 200, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = np.load('../dataPreprocessing/data/Fullpick_ETL/startified_sample/features/fullpick_chunk1.npy')\n",
    "chunk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-season",
   "metadata": {},
   "source": [
    "### Join all startified samples into one train/test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "varied-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "features_path = '../dataPreprocessing/data/Fullpick_ETL/startified_sample/features/'\n",
    "labels_path = '../dataPreprocessing/data/Fullpick_ETL/startified_sample/labels/'\n",
    "\n",
    "filenames = sorted(os.listdir('../dataPreprocessing/data/Fullpick_ETL/startified_sample/features'), key = lambda x : int(re.findall(r'\\d+',x)[0]) )\n",
    "\n",
    "features_names = [ features_path+filename for filename in filenames ]\n",
    "label_names = [ labels_path+filename for filename in filenames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "understanding-jesus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:21,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample length :  139397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "first  = True\n",
    "for feature_filename, label_filename in tqdm(zip(features_names, label_names)):\n",
    "    #load data\n",
    "    if first :\n",
    "        first  = False\n",
    "        data   = np.load(feature_filename, allow_pickle=True)\n",
    "        target = np.load(label_filename, allow_pickle=False)\n",
    "    \n",
    "    else : \n",
    "        data = np.append(data, np.load(feature_filename, allow_pickle=True), axis=0)\n",
    "        target = np.append(target, np.load(label_filename, allow_pickle=False))\n",
    "\n",
    "print('sample length : ', len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "otherwise-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139397, 200, 6)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "signal-pavilion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139397,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "likely-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_path = '../dataPreprocessing/data/Fullpick_ETL/startified_sample/merged_sample/'\n",
    "\n",
    "np.save(extraction_path+'features', data)\n",
    "np.save(extraction_path+'labels', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-float",
   "metadata": {},
   "source": [
    "### Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "incorporate-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../dataPreprocessing/data/Fullpick_ETL/startified_sample/merged_sample/features.npy', allow_pickle=True)\n",
    "target = np.load('../dataPreprocessing/data/Fullpick_ETL/startified_sample/merged_sample/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "numerous-merchandise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139397, 200, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "alert-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture keras API\n",
    "\n",
    "def generate_model(hp=None):\n",
    "    #default hyper parameters\n",
    "    num_stacked_dense = 1\n",
    "    droput_rate       = 0.2\n",
    "    gru_units         = 128\n",
    "    dense_units       = 128\n",
    "    learning_rate     = 0.002\n",
    "    num_stacked_gru   = 0\n",
    "    leakyRelu_alpha   = 0\n",
    "    num_stacked_convolution = 1\n",
    "    num_convolution_kernels = 32\n",
    "    \n",
    "    if hp:\n",
    "        num_stacked_dense = hp.Choice('num_stacked_dense', values=[1, 2, 3])\n",
    "        num_stacked_gru   = hp.Choice('num_stacked_gru', values=[0, 1])\n",
    "        leakyRelu_alpha   = hp.Float('leakyRelu_alpha', min_value=0 , max_value=0.3) \n",
    "        droput_rate       = hp.Float('droput_rate', min_value=0.1 , max_value=0.5)\n",
    "        learning_rate     = hp.Float('learning_rate', min_value=0.001, max_value=0.01)\n",
    "        gru_units         = hp.Int('gru_units', min_value=64, max_value=256, step=32)\n",
    "        dense_units       = hp.Int('dense_units', min_value=128, max_value=320, step=32)\n",
    "        num_stacked_convolution = hp.Choice('num_stacked_convolution', values=[1, 2, 3])\n",
    "        num_convolution_kernels = hp.Int('num_convolution_kernels', min_value=16, max_value=64, step=16)\n",
    "    \n",
    "    inputs = layers.Input(shape = (200,6))\n",
    "\n",
    "    #CNN + Maxpool block\n",
    "    x_conv = inputs[:4]\n",
    "    for i in range(num_stacked_convolution):\n",
    "        if i == 0 :\n",
    "            x_conv = layers.Conv1D(num_convolution_kernels, 3, activation='relu',input_shape=(-1,200,6))(x_conv)\n",
    "        else :\n",
    "            x_conv = layers.Conv1D(num_convolution_kernels, 3, activation='relu')(x_conv)            \n",
    "        x_conv = layers.Conv1D(num_convolution_kernels, 3, activation='relu' )(x_conv)\n",
    "        x_conv = layers.MaxPooling1D(2)(x_conv)\n",
    "    x_conv = layers.Flatten()(x_conv)\n",
    "    \n",
    "    #GRU layers Block\n",
    "    if num_stacked_gru > 0 :\n",
    "        x = layers.GRU(units=gru_units, activation='tanh' , input_shape=(-1, 200, 6), return_sequences=True )(inputs)\n",
    "        for i in range(num_stacked_gru):\n",
    "            if i+1 == num_stacked_gru:\n",
    "                x = layers.GRU(units = gru_units, activation='tanh' , return_sequences = False )(inputs)\n",
    "            else:\n",
    "                x = layers.GRU(units=gru_units, activation='tanh', return_sequences=True )(x)\n",
    "    else :\n",
    "        x = layers.GRU(units = gru_units, activation='tanh' , input_shape = (-1, 200, 6), return_sequences = False )(inputs)\n",
    "    \n",
    "    #reduce output size of convoulution block outputs same as the gru outputs\n",
    "    x_conv = layers.Dense(gru_units, activation='linear')(x_conv)\n",
    "    x_conv = layers.BatchNormalization()(x_conv)\n",
    "    x_conv = tf.nn.leaky_relu(x_conv, leakyRelu_alpha)\n",
    "\n",
    "    x_concat = layers.concatenate([x_conv,x], axis=1)\n",
    "    \n",
    "    #Dense layers block\n",
    "    for i in range(num_stacked_dense):\n",
    "        x = layers.Dense(dense_units, activation='linear')(x_concat)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = tf.nn.leaky_relu(x, leakyRelu_alpha)\n",
    "        x = layers.Dropout(droput_rate)(x)\n",
    "    #output layer\n",
    "    out = layers.Dense(43, activation='softmax')(x)\n",
    "    \n",
    "    #build model\n",
    "    model = Model(inputs, out)\n",
    "    #model compilation\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    loss = SparseCategoricalCrossentropy(from_logits = True)\n",
    "    metric = SparseCategoricalAccuracy()\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metric)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "balanced-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    \n",
    "    #run trial override\n",
    "    def run_trial(self, trial, X, y, epochs=1, verbose=True, callbacks=None, batch_size=512, k=5):\n",
    "        \n",
    "        val_losses = []        \n",
    "        \n",
    "        kf = KFold(n_splits=k)\n",
    "        k_iter = 1\n",
    "        for train_idx, val_idx in kf.split(X):            \n",
    "            gc.collect()\n",
    "            print('')\n",
    "            print('------KFold--CrossValidation--iteration--N°{}------'.format(k_iter), end='\\n\\n')\n",
    "            k_iter+=1\n",
    "            #kfold cross-validation split\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            #tensorflow data pipeline\n",
    "            train_set, validation_set = self.__tensorflow_dataPipeLine(X_train, y_train, X_val, y_val, batch_size)\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(train_set,\n",
    "                      validation_data=validation_set,\n",
    "                      epochs=epochs,\n",
    "                      verbose = verbose,\n",
    "                      callbacks=callbacks)\n",
    "\n",
    "            val_losses.append(max(hist.history['val_sparse_categorical_accuracy'])) \n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {'val_sparse_categorical_accuracy':np.mean(val_losses)})\n",
    "        self.save_model(trial.trial_id, model)\n",
    "        gc.collect()\n",
    "        \n",
    "    def __tensorflow_dataPipeLine(self, X_train, y_train, X_val, y_val, batch_size):\n",
    "\n",
    "        dataset = Dataset.from_tensor_slices((X_train, y_train))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.shuffle(1000).repeat(2)\n",
    "        train_set = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        dataset = Dataset.from_tensor_slices((X_val, y_val))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.shuffle(1000).repeat(2)\n",
    "        validation_set = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        return train_set, validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "complicated-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = CVTuner(\n",
    "        hypermodel=generate_model,\n",
    "        directory = './tuner',\n",
    "        project_name = 'path_perdiction',\n",
    "        overwrite = True,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "        objective = kt.Objective(\"val_sparse_categorical_accuracy\", direction=\"max\"),\n",
    "        max_trials=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cellular-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 9\n",
      "num_stacked_dense (Choice)\n",
      "{'default': 1, 'conditions': [], 'values': [1, 2, 3], 'ordered': True}\n",
      "num_stacked_gru (Choice)\n",
      "{'default': 0, 'conditions': [], 'values': [0, 1], 'ordered': True}\n",
      "leakyRelu_alpha (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': None, 'sampling': None}\n",
      "droput_rate (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': None, 'sampling': None}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.01, 'step': None, 'sampling': None}\n",
      "gru_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 320, 'step': 32, 'sampling': None}\n",
      "num_stacked_convolution (Choice)\n",
      "{'default': 1, 'conditions': [], 'values': [1, 2, 3], 'ordered': True}\n",
      "num_convolution_kernels (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(features, target, epochs=30, k=5, batch_size=64,\n",
    "             callbacks=[EarlyStopping(restore_best_weights=True, patience=3, monitor='val_sparse_categorical_accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model\n",
    "path_model.save_weights('./checkpoint/path_prediction.cpkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-mixture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-court",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
